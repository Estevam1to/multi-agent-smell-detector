import struct
import torch
import numpy as np

"""
Brian D. Zimmerman
August 8, 2023

This file contains functions for converting between different floating point formats.
Comments were generated by Github Copilot.
"""

# IEEE 754-2008 Standard for Floating-Point Arithmetic
IEEE_BIT_FORMATS = {
    torch.float32:{'exponent':8, 'mantissa':23, 'workable': torch.int32},
    torch.float16:{'exponent':5, 'mantissa':10, 'workable': torch.float},
    torch.bfloat16:{'exponent':8, 'mantissa':7, 'workable': torch.int32}
}


def print_bits(x : torch.Tensor, bits : int):
    """Prints the bits of a tensor

    Args:
        x (torch.Tensor): The tensor to print
        bits (int): The number of bits to print

    Returns:
        ByteTensor : The bits of the tensor
    """
    mask = 2**torch.arange(bits-1,-1,-1).to(x.device, x.dtype)
    print(x.unsqueeze(-1).bitwise_and(mask).ne(0).byte())


def shift_base(exponent_bits : int):
    """Computes the bias for a given number of exponent bits

    Args:
        exponent_bits (int): The number of exponent bits

    Returns:
        int : The bias for the given number of exponent bits
    """
    return 2 ** (exponent_bits - 1) - 1


def shift_right(t : torch.Tensor, shift_by : int):
    """Shifts a tensor to the right by a given number of bits

    Args:
        t (torch.Tensor): The tensor to shift
        shift_by (int): The number of bits to shift by

    Returns:
        torch.Tensor : The shifted tensor
    """
    return t >> shift_by


def shift_left(t : torch.Tensor, shift_by : int):
    """Shifts a tensor to the left by a given number of bits

    Args:
        t (torch.Tensor): The tensor to shift
        shift_by (int): The number of bits to shift by

    Returns:
        torch.Tensor : The shifted tensor
    """
    return t << shift_by


def fp8_downcast(source_tensor : torch.Tensor, n_bits : int):
    """Downcasts a tensor to an 8 bit float

    Args:
        source_tensor (torch.Tensor): The tensor to downcast
        n_bits (int): The number of bits to use for the mantissa

    Raises:
        ValueError: If the mantissa is too large for an 8 bit float

    Returns:
        ByteTensor : The downcasted tensor
    """
    target_m_nbits = n_bits
    target_e_nbits = 7 - n_bits

    if target_e_nbits + target_m_nbits + 1 > 8:
        raise ValueError("Mantissa is too large for an 8 bit float")
    
    source_m_nbits = IEEE_BIT_FORMATS[source_tensor.dtype]['mantissa']
    source_e_nbits = IEEE_BIT_FORMATS[source_tensor.dtype]['exponent']
    source_all_nbits = 1 + source_m_nbits + source_e_nbits
    int_type = torch.int32 if source_all_nbits == 32 else torch.int16
    
    #Extract the sign
    sign = shift_right(source_tensor.view(int_type), source_all_nbits - 1).to(torch.uint8)
    sign = torch.bitwise_and(sign, torch.ones_like(sign, dtype=torch.uint8))

    #Zero out the sign bit
    bit_tensor = torch.abs(source_tensor)

    #Shift the base to the right of the buffer to make it an int
    base = shift_right(bit_tensor.view(int_type), source_m_nbits)

    #Shift the base back into position and xor it with bit tensor to get the mantissa by itself
    mantissa = torch.bitwise_xor(shift_left(base, source_m_nbits), bit_tensor.view(int_type))
    
    #Shift the mantissa left by the target mantissa bits then use modulo to zero out anything outside of the mantissa
    t1 = (shift_left(mantissa,target_m_nbits) % (2**source_m_nbits))
    #Create a tensor of fp32 1's and convert them to int32
    t2 = torch.ones_like(source_tensor).view(int_type)
    #Use bitwise or to combine the 1-floats with the shifted mantissa to get the probabilities + 1 and then subtract 1
    expectations = (torch.bitwise_or(t1,t2).view(source_tensor.dtype) - 1)

    #Stochastic rounding
    #torch.ceil doesnt work on float16 tensors
    #https://github.com/pytorch/pytorch/issues/51199
    r = torch.rand_like(expectations,dtype=torch.float32)
    ones = torch.ceil(expectations.to(torch.float32) - r).type(torch.uint8)

    #Shift the sign, base, and mantissa into position
    target_sign = shift_left(sign,7)
    target_base = base - shift_base(source_e_nbits) + shift_base(target_e_nbits)
    target_base = shift_left(target_base, target_m_nbits).to(torch.uint8)
    target_mantissa = shift_right(mantissa, source_m_nbits - target_m_nbits).to(torch.uint8)
    fp8_as_uint8 = target_sign + target_base + target_mantissa
    
    return fp8_as_uint8 + ones


def uint8_to_fp16(source_tensor : torch.ByteTensor, n_bits : int):
    """Converts a uint8 tensor to a fp16 tensor

    Args:
        source_tensor (torch.ByteTensor): The tensor to convert
        n_bits (int): The number of bits to use for the mantissa

    Returns:
        _type_: The converted tensor
    """
    source_m_nbits = n_bits
    source_e_nbits = 7 - n_bits
    
    #Extract sign as int16
    sign = shift_right(source_tensor, 7)
    shifted_sign = shift_left(sign.type(torch.int16), 15)
    
    #Extract base as int16 and adjust the bias accordingly
    base_mantissa = shift_left(source_tensor, 1)
    base = shift_right(base_mantissa, source_m_nbits + 1) - shift_base(source_e_nbits)
    base = base.type(torch.int16) + shift_base(5)
    shifted_base = shift_left(base, 10)

    #Extract mantissa as int16
    mantissa = shift_left(base_mantissa, source_e_nbits)
    shifted_mantissa = shift_left(mantissa.type(torch.int16),2)
    
    return (shifted_base + shifted_sign + shifted_mantissa).view(torch.float16)